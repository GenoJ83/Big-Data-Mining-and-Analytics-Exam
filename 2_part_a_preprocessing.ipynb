{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part A: Big Data Platform Setup and Data Preprocessing\n",
                "\n",
                "**Course**: DSC3108 - Big Data Mining and Analytics  \n",
                "**Scenario**: Large-Scale Retail Recommendation System"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Big Data Justification (50 words)\n",
                "\n",
                "The Retail Recommendation scenario involves processing **high-volume transactional data** (millions of rows) with **high velocity** (real-time purchases). Relational databases struggle with such scale and unstructured correlations. A Big Data platform like **Apache Spark** is necessary for distributed processing, enabling scalable collaborative filtering and real-time personalized recommendations."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Tool Selection: Apache Spark (PySpark)\n",
                "\n",
                "We use **PySpark** for distributed data processing because:\n",
                "- In-memory computation for fast iterative algorithms\n",
                "- Built-in MLlib for scalable machine learning\n",
                "- Handles data partitioning across nodes automatically"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import SparkSession\n",
                "from pyspark.sql.functions import col, to_timestamp\n",
                "\n",
                "# Initialize Spark Session\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"RetailRecommendation_Preprocessing\") \\\n",
                "    .config(\"spark.driver.memory\", \"2g\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "spark.sparkContext.setLogLevel(\"ERROR\")\n",
                "print(f\"✓ Spark {spark.version} initialized\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Acquisition\n",
                "\n",
                "Load the generated CSV files into Spark DataFrames."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load transactions data\n",
                "df = spark.read.csv(\"transactions.csv\", header=True, inferSchema=True)\n",
                "\n",
                "print(f\"Initial raw count: {df.count():,}\")\n",
                "df.show(5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check schema\n",
                "df.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Distributed Processing: Data Cleaning and Transformation\n",
                "\n",
                "### 4.1 Remove Duplicates"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_clean = df.dropDuplicates()\n",
                "print(f\"Removed {df.count() - df_clean.count():,} duplicate rows\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Handle Missing Values"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df_clean = df_clean.dropna()\n",
                "print(f\"Cleaned count: {df_clean.count():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Data Type Transformations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Convert timestamp to proper datetime type\n",
                "df_clean = df_clean.withColumn(\"timestamp\", to_timestamp(col(\"timestamp\")))\n",
                "\n",
                "# Ensure correct data types\n",
                "df_clean = df_clean.withColumn(\"user_id\", col(\"user_id\").cast(\"integer\")) \\\n",
                "                   .withColumn(\"product_id\", col(\"product_id\").cast(\"integer\")) \\\n",
                "                   .withColumn(\"rating\", col(\"rating\").cast(\"float\"))\n",
                "\n",
                "print(\"✓ Data types corrected\")\n",
                "df_clean.printSchema()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Data Quality Verification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary statistics\n",
                "df_clean.select(\"rating\").summary().show()\n",
                "\n",
                "# Check for any remaining nulls\n",
                "from pyspark.sql.functions import col, sum as spark_sum\n",
                "null_counts = df_clean.select([spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_clean.columns])\n",
                "null_counts.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save Cleaned Data (Optional)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save as Parquet for efficient storage and faster loading in Part B\n",
                "df_clean.write.mode(\"overwrite\").parquet(\"transactions_clean.parquet\")\n",
                "print(\"✓ Cleaned data saved to transactions_clean.parquet\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "**Part A Completed:**\n",
                "- ✓ Justified Big Data approach\n",
                "- ✓ Set up Apache Spark platform\n",
                "- ✓ Acquired and ingested data\n",
                "- ✓ Performed distributed cleaning and transformation\n",
                "\n",
                "**Next**: Proceed to Part B for model development."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}